%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{One-Sided Tests}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{One-sided Test: Different Decision Rule} 

  \begin{block}{Same Example as Last Time}
  $X_1, \dots, X_{100}\sim \mbox{iid N}(\mu, 9)$ and $H_0\colon \mu=2$. 
\end{block}

\begin{alertblock}{Three possible alternatives:}

  \begin{columns}
    \column{0.23\textwidth}
    \begin{block}{Two-sided}
      $H_1\colon \mu \neq 2$
    \end{block}
    \column{0.23\textwidth}
    \begin{block}{One-sided $(<)$}
      $H_1\colon \mu < 2$
    \end{block}
    \column{0.23\textwidth}
    \begin{block}{One-sided $(>)$}
      $H_1\colon \mu > 2$
    \end{block}
  \end{columns}
\end{alertblock}

\vspace{1em}

\begin{block}{Three corresponding decision rules:}
  \begin{itemize}
    \item Two-sided: reject $\mu = 2$ whenever $|\bar{X}_n - 2|$ is too large.
    \item One-sided $(<)$: only reject $\mu = 2$ if $\bar{X}_n$ is far \alert{below} 2.
    \item One-sided $(>)$: only reject $\mu = 2$ if $\bar{X}_n$ is far \alert{above} 2.
  \end{itemize}
\end{block}
  %These are stranger since the analogy with a confidence interval breaks down.
  %Explain that the default should be a two-sided test.
  %Then explain why and when you might want to do a one-sided test.
  %It's about \emph{power} which we'll cover in more detail in the next lecture. 
  %But the basic idea is that you're more likely to reject if the null is false.
  %Show an example.
  %Then explain about a one-sided p-value.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{One-sided $(>)$ Example: $X_1, \dots, X_{100} \sim \mbox{iid N}(\mu,9)$}
  \begin{block}{Null and Alternative}
  Test $H_0\colon \mu =2$ against $H_0\colon \mu >2$ with $\alpha = 0.05$. 
  \end{block}

  \pause

  \begin{block}{Test Statistic}
    Drop absolute value for one-sided test: $\displaystyle T_n = \frac{\bar{X}_n - 2}{0.3}$ 
  \end{block}

  \pause

  \begin{block}{Decision Rule}
    Reject $H_0\colon \mu =2$ if test statistic is \alert{large and positive}: $T_n > c$ 
  \end{block}

  \pause

  \begin{block}{Critical Value}
    Choose $c$ so that $P(\mbox{type I error}) = P(T_n > c|\mu =2) = 0.05$ 
  \end{block}

  \pause

  \begin{block}{Under $H_0$, $T_n \sim N(0,1)$}
   If $Z\sim N(0,1)$ what value of $c$ ensures $P(Z>c) = 0.05$? 
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{One-sided $(<)$ Example: $X_1, \dots, X_{100} \sim \mbox{iid N}(\mu,9)$}
  \begin{block}{Null and Alternative}
  Test $H_0\colon \mu =2$ against $H_1\colon \mu <2$ with $\alpha = 0.05$. 
  \end{block}

  \pause

  \begin{block}{Test Statistic}
    Drop absolute value for one-sided test: $\displaystyle T_n = \frac{\bar{X}_n - 2}{0.3}$ 
  \end{block}

  \pause

  \begin{block}{Decision Rule}
    Reject $H_0\colon \mu =2$ if test statistic is \alert{large and negative}: $T_n < c$ 
  \end{block}

  \pause

  \begin{block}{Critical Value}
    Choose $c$ so that $P(\mbox{type I error}) = P(T_n < c|\mu =2) = 0.05$ 
  \end{block}

  \pause

  \begin{block}{Under $H_0$, $T_n \sim N(0,1)$}
   If $Z\sim N(0,1)$ what value of $c$ ensures $P(Z<c) = 0.05$? 
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Critical Values -- Two-sided vs.\ One-sided Tests: $\alpha = 0.05$}

  \small

  \vspace{-1em}

  \begin{figure}
    \centering
    \includegraphics[scale=0.5]{./images/onesided_vs_twosided.pdf}
  \end{figure}

  \vspace{-2em}

  \begin{block}{Two-Sided}
    Splits $\alpha=0.05$ between two tails: $c = \texttt{qnorm}(1 - 0.05/2) \approx 1.96$ 
  \end{block}

  \begin{block}{One-Sided}
    One tail: $c = \texttt{qnorm}(0.05)\approx  -1.64$ for ($<$); $\texttt{qnorm}(0.95) \approx 1.64$ for ($>$) 
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Example: $X_1, \dots, X_{100} \sim \mbox{iid N}(\mu,9), \alpha = 0.05$}

  \small

  \[\boxed{\mbox{Suppose } \bar{x} = 1.5 \implies (\bar{x} - 2)/0.3 \approx -1.67}\]


  \pause

  \begin{columns}
    \column{0.32\textwidth}
    \begin{block}{Two-sided}
      $H_1\colon \mu \neq 2$\\
      Reject if $|T_n| > 1.96$\\
      $T_n = 1.67$\\
      \alert{Fail to reject}
    \end{block}
    \column{0.32\textwidth}
    \begin{block}{One-sided $(<)$}
      $H_1\colon \mu < 2$\\
      Reject if $T_n < -1.64$\\
      $T_n = -1.67$\\
      \textcolor{blue}{Reject}
    \end{block}
    \column{0.32\textwidth}
    \begin{block}{One-sided $(>)$}
      $H_1\colon \mu > 2$\\
      Reject if $T_n > 1.64$\\
      $T_n = -1.67$\\
      \alert{Fail to reject}
    \end{block}
  \end{columns}

  \pause

  \vspace{2em}
  \begin{itemize}
    \item If One-sided $(<)$ rejects, then one-sided $(>)$ doesn't and vice-versa. \pause
    \item Two-sided and one-sided sometimes agree but sometimes disagree. \pause
    \item One-sided test is ``less stringent.''
  \end{itemize}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Testing $H_0\colon \mu = \mu_0$ when $X_1, \dots, X_n \sim \mbox{iid N}(\mu, \sigma^2)$}

  \begin{block}{Two-Sided}
    Reject $H_0$ whenever $\displaystyle \left|\frac{\bar{X}_n - \mu_0}{\sigma/\sqrt{n}}\right|> \texttt{qnorm}(1 - \alpha/2)$ 
  \end{block}

  \begin{block}{One-Sided $(<)$}
    Reject $H_0$ whenever $\displaystyle \frac{\bar{X}_n - \mu_0}{\sigma/\sqrt{n}} < \texttt{qnorm}(\alpha)$ 
  \end{block}

  \begin{block}{One-Sided $(>)$}
    Reject $H_0$ whenever $\displaystyle \frac{\bar{X}_n - \mu_0}{\sigma/\sqrt{n}} > \texttt{qnorm}(1 - \alpha)$ 
  \end{block}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{One-sided P-value}

  \small

  \begin{itemize}
    \item Only makes sense to calculate one-sided p-value when sign of test stat.\ agrees with alternative: \pause
      \begin{itemize}
        \item Preceding example: $T_n = -1.67$  \pause
        \item Calculate p-value for test vs.\ $H_1\colon \mu < 2$ but \alert{not $H_1\colon \mu >2$} \pause
      \end{itemize}
    \item Just as in two-sided test, p-value equals value of $\alpha$ for which $c$ exactly equals the observed test statistic: \pause
      \begin{itemize}
        \item $c = \texttt{qnorm}(\alpha)$ for $(<)$ \pause
        \item $c = \texttt{qnorm}(1 - \alpha)$ for $(>)$ \pause
        \item Example: $-1.67 = \texttt{qnorm}(\alpha) \iff \alpha = 0.047$ \pause
      \end{itemize}
    \item Use and report  one-sided p-value in same way as two-sided p-value
  \end{itemize}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Comparing One-sided and Two-sided Tests}

  \begin{itemize}
    \item Two-sided test is the default. \pause
    \item Don't use one-sided unless you have a good reason! \pause
    \item Relationship between CI and test \alert{only holds for two-sided}.\pause
    \item Why and when should we consider a one-sided test?\pause
      \begin{itemize}
        \item Suppose we know \emph{a priori} that $\mu < 2$ is crazy/uninteresting\pause
        \item Test of $H_0\colon \mu=2$ against $H_1\colon \mu>2$ with significance level $\alpha$ has \alert{lower type II error rate} than test against $H_1\colon \mu\neq 2$. \pause
      \end{itemize}
    \item If you use a one-sided test you \alert{must choose $(>)$ or $(<)$ \emph{before looking at the data}}. Otherwise the results are invalid.
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Roadmap}
%  \small
%
%  \begin{block}{Next Time}
%    More examples of hypothesis testing using relationship to CIs to help us avoid re-inventing the wheel.
%  \end{block}
%
%
%  \pause
%
%  \begin{block}{Building Intuition}
%   Now that you know a simple example of a hypothesis test and its relationship to a CI, think about the following:
%   \begin{itemize}
%     \item If we reject $H_0$ does that mean that $H_0$ is false?
%     \item How does testing relate to random sampling?
%      \item How does critical value of two-sided test relate to width of CI?
%      \item In a given test, which is larger: the one-sided or two-sided p-value?
%   \end{itemize}
%  \end{block}
%
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{More Examples}
%  Re-do all the CI examples as tests\dots
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{The Anchoring Experiment}
\begin{figure}
\centering
\includegraphics[scale = 0.55]{./images/anchoring_boxplot}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Two-Sample Test For Difference of Means}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{The Anchoring Experiment}
Shown a ``random'' number and then asked what proportion of UN member states are located in Africa.
	\begin{block}{``Hi'' Group -- Shown 65 ($n_{Hi}=46$)}
		Sample Mean: $30.7$, Sample Variance: $253$
\end{block}


	\begin{block}{``Lo'' Group -- Shown 10 ($n_{Lo}=43$)}
	Sample Mean: $17.1$, Sample Variance: $86$
\end{block}


\vspace{1em}

\hfill\alert{\fbox{Proceed via the CLT...}}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
	\frametitle{In words, what is our null hypothesis?\hfill \includegraphics[scale = 0.05]{./images/clicker}}

	\begin{enumerate}[(a)]
		\item There is a \emph{positive} anchoring effect: seeing a higher random number makes people report a higher answer.
		\item There is a \emph{negative} anchoring effect: seeing a lower random number makes people report a lower answer. 
		\item There \emph{is} an anchoring effect: it could be positive or negative.
		\item There is  \emph{no} anchoring effect: people aren't influenced by seeing a random number before answering.
	\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{In symbols, what is our null hypothesis?\hfill \includegraphics[scale = 0.05]{./images/clicker}}

	\begin{enumerate}[(a)]
		\item $\mu_{Lo} < \mu_{Hi}$
		\item $\mu_{Lo} = \mu_{Hi}$
		\item $\mu_{Lo} > \mu_{Hi}$
		\item $\mu_{Lo} \neq \mu_{Hi}$
	\end{enumerate}
\pause
	\vspace{1em}

	\alert{$\mu_{Lo} = \mu_{Hi}$ is \emph{equivalent to} $\mu_{Hi} - \mu_{Lo} = 0$!}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Anchoring Experiment\hfill \includegraphics[scale = 0.05]{./images/clicker}}
 
Under the null, what should we expect to be true about the values taken on by $\bar{X}_{Lo}$ and $\bar{X}_{Hi}$?

\vspace{1em}

	\begin{enumerate}[(a)]
		\item They should be similar in value.
		\item $\bar{X}_{Lo}$ should be the smaller of the two.
		\item $\bar{X}_{Hi}$ should be the smaller of the two.
		\item They should be different. We don't know which will be larger.
	\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{What is our Test Statistic?}
\begin{block}{Sampling Distribution}
		$$\frac{\left(\bar{X}_{Hi} - \bar{X}_{Lo}\right) - \left(\mu_{Hi} - \mu_{Lo}\right)}{\sqrt{\frac{S_{Hi}^2}{n_{Hi}} + \frac{S_{Lo}^2}{n_{Lo}}}} \approx N(0,1)$$
\end{block}

\begin{block}{Test Statistic: Impose the Null}
Under $H_0\colon \mu_{Lo} = \mu_{Hi}$
	$$T_n =\frac{\bar{X}_{Hi} - \bar{X}_{Lo}}{\sqrt{\frac{S_{Hi}^2}{n_{Hi}} + \frac{S_{Lo}^2}{n_{Lo}}}} \approx N(0,1)$$
\end{block}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{What is our Test Statistic?}
\footnotesize
$\bar{X}_{Hi} = 30.7$, $s^2_{Hi} = 253$, $n_{Hi} = 46$\\
$\bar{X}_{Lo} = 17.1$, $s^2_{Lo} = 86$, $n_{Lo} = 43$\\
\normalsize
\vspace{2em}

\begin{block}{Under $H_0\colon \mu_{Lo} = \mu_{Hi}$}
	$$T_n = \frac{\bar{X}_{Hi} - \bar{X}_{Lo}}{\sqrt{\frac{S_{Hi}^2}{n_{Hi}} + \frac{S_{Lo}^2}{n_{Lo}}}} \approx N(0,1)$$
\end{block}

\begin{block}{Plugging in Our Data}
$$T_n = \frac{\bar{X}_{Hi} - \bar{X}_{Lo}}{\sqrt{\frac{S_{Hi}^2}{n_{Hi}} + \frac{S_{Lo}^2}{n_{Lo}}}} \approx 5$$
\end{block}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]
\frametitle{Anchoring Experiment Example \hfill \includegraphics[scale = 0.05]{./images/clicker}}
Approximately what critical value should we use to test $H_0\colon \mu_{Lo} = \mu_{Hi}$ against the two-sided alternative at the 5\% significance level?

\pause
\vspace{1em}
\begin{center}
\begin{tabular}{l|lll}
$\alpha$ &   0.10& 0.05 &0.01\\
\hline
\texttt{qnorm($1-\alpha$)} & 1.28 &1.64 &2.33\\
\texttt{qnorm($1-\alpha/2$)} &1.64 &\alert{1.96}& 2.58
\end{tabular}
\end{center}
\hfill \alert{... Approximately 2}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Anchoring Experiment Example\hfill \includegraphics[scale = 0.05]{./images/clicker}}
Which of these commands would give us the p-value of our test of $H_0\colon \mu_{Lo} = \mu_{Hi}$ against $H_1\colon \mu_{Lo}<\mu_{Hi}$ at significance level $\alpha$?
\vspace{1em}
	\begin{enumerate}[(a)]
		\item \texttt{qnorm($1-\alpha$)}
		\item \texttt{qnorm($1-\alpha/2$)}
		\item \texttt{1 - pnorm(5)}
		\item \texttt{2 * (1 - pnorm(5))}
	\end{enumerate}
	

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{P-values for $H_0\colon \mu_{Lo} = \mu_{Hi}$}
We plug in the value of the test statistic that we observed: 5
\begin{block}{Against $H_1\colon \mu_{Lo}< \mu_{Hi}$}
\texttt{1 - pnorm(5)} $< 0.0000$
\end{block}

\begin{block}{Against $H_1\colon \mu_{Lo}\neq \mu_{Hi}$}
\texttt{2 * (1 - pnorm(5))} $< 0.0000$
\end{block}

\vspace{1em}

\alert{If the null is true (the two population means are equal) it would be extremely unlikely to observe a test statistic as large as this!}

\vspace{1em} 
\hfill \fbox{What should we conclude?}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Matched Pairs Test for Difference of Means}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Which Exam is Harder?}
% latex.default(round(grades.out, 1), file = "grades.tex", rowname = NULL) 
%
\begin{table}[!tbp]
\begin{center}
\begin{tabular}{rccr}
\hline\hline
\multicolumn{1}{r}{Student}&\multicolumn{1}{c}{Exam 1}&\multicolumn{1}{c}{Exam 2}&\multicolumn{1}{r}{Difference}\tabularnewline
\hline
$ 1$&$57.1$&$60.7$&$  3.6$\tabularnewline
\vdots&\vdots&\vdots&\vdots\\
$71$&$78.6$&$82.9$&$  4.3$\tabularnewline
\hline
Sample Mean: & 79.6 & 81.4  &1.8\\
Sample Var. &117  & 151 & 124\\
Sample Corr.& \multicolumn{2}{c}{0.54}&\\
\hline
\end{tabular}
\end{center}
\end{table}

\vspace{1em}

\fbox{Again, we'll use the CLT.}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{One-Sample Hypothesis Test Using Differences}
\small
\fbox{Let $D_i = X_i - Y_i$ be (Midterm 2 Score - Midterm 1 Score) for student $i$}
\vspace{0.1em}
\begin{block}{Null Hypothesis}
$H_0\colon \mu_1 = \mu_2$, i.e.\ both exams were of the same difficulty
\end{block}
\begin{block}{Two-Sided Alternative}
$H_1\colon \mu_1 \neq \mu_2$, i.e.\ one exam was harder than the other
\end{block}
\begin{block}{One-Sided Alternative}
$H_1\colon \mu_2 > \mu_1$, i.e.\ the second exam was easier
\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Decision Rules}
\small
\fbox{Let $D_i = X_i - Y_i$ be (Midterm 2 Score - Midterm 1 Score) for student $i$}
\vspace{0.1em}


\begin{block}
	{Test Statistic}
$$\displaystyle \frac{\bar{D}_n}{\widehat{SE}(\bar{D}_n)}=\frac{1.8}{\sqrt{124/71}} \approx 1.36$$
\end{block}


\begin{block}{Two-Sided Alternative} 
Reject $H_0\colon \mu_1 = \mu_2$ in favor of $H_1\colon \mu_1 \neq \mu_2$ if $|\bar{D}_n|$ is sufficiently large.
\end{block}
\begin{block}{One-Sided Alternative}
Reject $H_0\colon \mu_1 = \mu_2$ in favor of $H_1\colon \mu_2 >\mu_1$ if $\bar{D}_n$ is sufficiently large.
\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Reject against \emph{Two-sided} Alternative with $\alpha = 0.1$?  \includegraphics[scale = 0.05]{./images/clicker}}

	$$\boxed{\displaystyle \frac{\bar{D}_n}{\widehat{SE}(\bar{D}_n)}= \frac{1.8}{\sqrt{124/71}} \approx 1.36} $$

\begin{center}
\begin{tabular}{l|lll}
$\alpha$ &   0.10& 0.05 &0.01\\
\hline
\texttt{qnorm($1-\alpha$)} & 1.28 &1.64 &2.33\\
\texttt{qnorm($1-\alpha/2$)} &1.64 &1.96& 2.58
\end{tabular}
\end{center}

\begin{enumerate}[(a)]
\item Reject
\item Fail to Reject
\item Not Sure
\end{enumerate}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Reject against \emph{One-sided} Alternative with $\alpha = 0.1$?  \includegraphics[scale = 0.05]{./images/clicker}}

	$$\boxed{\displaystyle \frac{\bar{D}_n}{\widehat{SE}(\bar{D}_n)}= \frac{1.8}{\sqrt{124/71}} \approx 1.36} $$

\begin{center}
\begin{tabular}{l|lll}
$\alpha$ &   0.10& 0.05 &0.01\\
\hline
\texttt{qnorm($1-\alpha$)} & 1.28 &1.64 &2.33\\
\texttt{qnorm($1-\alpha/2$)} &1.64 &1.96& 2.58
\end{tabular}
\end{center}

\begin{enumerate}[(a)]
\item Reject
\item Fail to Reject
\item Not Sure
\end{enumerate}



\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{P-Values for the Test of $H_0\colon \mu_1 = \mu_2$}

	$$\boxed{\displaystyle \frac{\bar{D}_n}{\widehat{SE}(\bar{D}_n)}= \frac{1.8}{\sqrt{124/71}} \approx 1.36} $$

\begin{block}{One-Sided $H_1\colon \mu_2 > \mu_1 $} 
$\texttt{1 - pnorm(1.36)} =  \texttt{pnorm(-1.36)}  \approx 0.09$ 
\end{block}

\begin{block}{Two-Sided $H_1 \colon \mu_1 \neq \mu_2$} 
$\texttt{2 * (1 - pnorm(1.36))} =  \texttt{2 * pnorm(-1.36)} \approx 0.18$
\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
